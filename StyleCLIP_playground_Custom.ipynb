{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "StyleCLIP-playground_Custom.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrober24/A.UD_289.84_seminar/blob/main/StyleCLIP_playground_Custom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J1lmK2Viqey"
      },
      "source": [
        "# Text-Guided Editing of Images (Using CLIP and StyleGAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "5wtomsoaVuhL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ISx91HUV0Nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874e7499-3306-4441-a2ac-537c1c3a3e16",
        "cellView": "form"
      },
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-df3fccac-d004-721d-b056-bbcf7fd06b34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-LU395da_Pe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e7d75e8-5c69-4fe0-b3b4-8e1891b04eca",
        "cellView": "form"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup\n",
        "import os\n",
        "\n",
        "!git clone https://github.com/dvschultz/stylegan2-ada-pytorch/\n",
        "!git clone https://github.com/rosinality/stylegan2-pytorch\n",
        "!pip install ninja\n",
        "\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "if os.path.isdir(\"/content/drive/MyDrive/Style_CLIP\"):\n",
        "  %cd \"/content/drive/MyDrive/Style_CLIP/StyleCLIP\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "  %cd \"/content/drive/MyDrive/\"\n",
        "  !mkdir Style_CLIP\n",
        "  %cd Style_CLIP\n",
        "  !git clone https://github.com/dvschultz/StyleCLIP.git\n",
        "  %cd StyleCLIP\n",
        "  !mkdir pkls\n",
        "  !mkdir pts\n",
        "else:\n",
        "  !git clone https://github.com/dvschultz/StyleCLIP.git\n",
        "  %cd StyleCLIP\n",
        "  !mkdir pkls\n",
        "  !mkdir pts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "mLdMP9GCbGqA",
        "outputId": "24c69e09-ad86-4a3f-fa89-25d685710e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 533, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 533 (delta 2), reused 0 (delta 0), pack-reused 524\u001b[K\n",
            "Receiving objects: 100% (533/533), 8.42 MiB | 11.02 MiB/s, done.\n",
            "Resolving deltas: 100% (303/303), done.\n",
            "Cloning into 'stylegan2-pytorch'...\n",
            "remote: Enumerating objects: 395, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 395 (delta 0), reused 1 (delta 0), pack-reused 392\u001b[K\n",
            "Receiving objects: 100% (395/395), 122.52 MiB | 19.29 MiB/s, done.\n",
            "Resolving deltas: 100% (200/200), done.\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 23.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.10.2.3\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-fngoyrm2\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-fngoyrm2\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369328 sha256=465776dcad9bc1ba11b51eb50e7d232db5f528ec72edd3df53b384ecfe43c2f5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-987agzzx/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n",
            "/content/drive/MyDrive/Style_CLIP/StyleCLIP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Conversion"
      ],
      "metadata": {
        "id": "5neXtv4TYJ2P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzZbVLj6T0Lz"
      },
      "source": [
        "This model requires a model file in the Rosinality format. You must convert your Pkl file to Pt format for this to run. You can do this with the next cell. Alternatively, you can use [this notebook](https://colab.research.google.com/github/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PT_to_Rosinality.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert Pkl to Pt\n",
        "\n",
        "%cd /content/stylegan2-ada-pytorch/\n",
        "\n",
        "pkl_filepath = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/forest_gothic.pkl' #@param {type: 'string'}\n",
        "pt_filepath = '/content/drive/MyDrive/Style_CLIP/StyleCLIP/pts/forest_gothic.pt' #@param {type: 'string'}\n",
        "\n",
        "!python export_weights.py $pkl_filepath $pt_filepath\n",
        "\n",
        "%cd /content/drive/MyDrive/Style_CLIP/StyleCLIP"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XceC6CG_Wkxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings"
      ],
      "metadata": {
        "id": "YH5VmBKPV3dB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTAVTULlq87j"
      },
      "source": [
        "experiment_type = 'free_generation' \n",
        "\n",
        "model_path = '/content/drive/MyDrive/Style_CLIP/StyleCLIP/pts/forest_gothic.pt' #@param {type:\"string\"}\n",
        "\n",
        "description = 'A gothic cathedral full of trees' #@param {type:\"string\"}\n",
        "\n",
        "latent_path = None \n",
        "\n",
        "optimization_steps = 500 #@param {type:\"number\"}\n",
        "\n",
        "l2_lambda = 0.008\n",
        "\n",
        "stylegan_size = \"512\" #@param ['256', '512', '1024']\n",
        "\n",
        "learning_rate = 0.2 #@param {type: 'slider', min: 0.01, max: 1, step: 0.01}\n",
        "\n",
        "truncation_psi = 0.7 #@param {type: 'slider', min:0, max:1, step:0.1}\n",
        "\n",
        "create_video = True #@param {type:\"boolean\"}\n",
        "\n",
        "args = {\n",
        "    \"description\": description,\n",
        "    \"ckpt\": model_path,\n",
        "    \"stylegan_size\": int(stylegan_size),\n",
        "    \"lr_rampup\": 0.05,\n",
        "    \"lr\": learning_rate, #default 0.1\n",
        "    \"step\": optimization_steps,\n",
        "    \"mode\": experiment_type,\n",
        "    \"l2_lambda\": l2_lambda,\n",
        "    \"latent_path\": latent_path,\n",
        "    \"truncation\": truncation_psi,\n",
        "    \"save_intermediate_image_every\": 1 if create_video else 20,\n",
        "    \"results_dir\": \"results\",\n",
        "    \"save_vector\": True\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "4EPNaw9kV-g1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT9JRl8hnT1l",
        "cellView": "form"
      },
      "source": [
        "#@title Run Optimization\n",
        "\n",
        "\n",
        "from optimization.run_optimization import main\n",
        "from argparse import Namespace\n",
        "result = main(Namespace(**args))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h15xcbHwnW0U",
        "cellView": "form"
      },
      "source": [
        "#@title Visualize Result\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms import ToPILImage\n",
        "result_image = ToPILImage()(make_grid(result.detach().cpu(), normalize=True, scale_each=True, range=(-1, 1), padding=0))\n",
        "h, w = result_image.size\n",
        "result_image.resize((h // 2, w // 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNRSSi0IcQID",
        "cellView": "form",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "#@title Create Video\n",
        "\n",
        "video_name = 'test' #@param {type: 'string'}\n",
        "vid_path = './' + video_name + '.mp4'\n",
        "\n",
        "!ffmpeg -r 15 -i ./results/%05d.png -c:v libx264 -vf fps=25 -pix_fmt yuv420p $vid_path"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}