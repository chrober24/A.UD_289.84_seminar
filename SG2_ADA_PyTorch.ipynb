{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of SG2-ADA-PyTorch.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrober24/A.UD_289.84_seminar/blob/main/SG2_ADA_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG7ZEc_982io"
      },
      "source": [
        "# StyleGAN2-ADA-PyTorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj4PG4_i9Alt"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGEXPcFJ9UTY"
      },
      "source": [
        "Check GPU (V100 best. P100 and K80 acceptible)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VVICTCvd4mc"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuVPuJmbigRs",
        "cellView": "form"
      },
      "source": [
        "#@title Connect Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ADVNpBh8Ox",
        "cellView": "form"
      },
      "source": [
        "#@title ## Install repo\n",
        "\n",
        "#@markdown The next cell will install the StyleGAN repository in Google Drive. \n",
        "#@markdown If you have already installed it it will just move into that folder. \n",
        "#@markdown If you don’t have Google Drive connected it will just install the necessary code in Colab.\n",
        "\n",
        "import os\n",
        "if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n",
        "    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !mkdir colab-sg2-ada-pytorch\n",
        "    %cd colab-sg2-ada-pytorch\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir generated\n",
        "    !mkdir datasets\n",
        "    !mkdir factors\n",
        "    !mkdir blended_models\n",
        "    !mkdir projection\n",
        "    %cd projection\n",
        "    !mkdir input\n",
        "    !mkdir output\n",
        "    %cd ../\n",
        "    !mkdir pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
        "    %cd ../\n",
        "else:\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir generated\n",
        "    !mkdir datasets\n",
        "    !mkdir factors\n",
        "    !mkdir blended_models\n",
        "    %cd projection\n",
        "    !mkdir input\n",
        "    !mkdir output\n",
        "    %cd ../\n",
        "    !mkdir pretrained\n",
        "    %cd pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n",
        "    %cd ../\n",
        "\n",
        "!pip install ninja opensimplex torch==1.7.1 torchvision==0.8.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZkcJ58P97Ls"
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "Upload a .zip of square images to the `datasets` folder. You can do this manually, or use the steps in the [Dataset_Prep notebook](https://github.com/chrober24/A.UD_289.84_seminar/blob/main/Dataset_Prep.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B-h6FpB9FaK"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV0W6yxP-UIn",
        "cellView": "form"
      },
      "source": [
        "#@title Training Settings\n",
        "#@markdown `daataset_name`: name of zipped folder of images in the datasets folder\n",
        "\n",
        "#@markdown `resume_from`: path to either pretrained model or most recent pkl from your results folder \n",
        "#@markdown (if you’re starting a new dataset I recommend `'ffhq1024'` or `'./pretrained/wikiart.pkl'`)\n",
        "\n",
        "#@markdown `mirror_x`: mirror augmentation of dataset. \n",
        "\n",
        "#@markdown `mirror_y`: vertical mirror augmentation of dataset. uncheck for images that have a specific orientation ie. faces\n",
        "\n",
        "#@markdown `snapshot_interval`: how often the training is saved (1 is best, but you can increase to save on Drive storage)\n",
        "dataset_name = 'example' #@param {type: 'string'}\n",
        "dataset_path = './datasets/' + dataset_name +'.zip'\n",
        "resume_from = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00016-insects_machines-mirror-11gb-gpu-gamma50-bg-resumeffhq1024/network-snapshot-000128.pkl' #@param {type: 'string'}\n",
        "aug_strength = 0.2\n",
        "train_count = 0\n",
        "mirror_x = True #@param {type: 'boolean'}\n",
        "mirror_y = False #@param {type: 'boolean'}\n",
        "\n",
        "#optional: you might not need to edit these\n",
        "gamma_value = 50.0\n",
        "augs = 'bg'\n",
        "config = '11gb-gpu'\n",
        "snapshot_interval = 1 #@param {type: 'slider', min: 1, max: 10}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL-M7WnnfMDI",
        "cellView": "form"
      },
      "source": [
        "#@title Run Training\n",
        "\n",
        "!python train.py --gpus=1 --cfg=$config --metrics=None --outdir=./results --data=$dataset_path --snap=$snapshot_interval --resume=$resume_from --augpipe=$augs --initstrength=$aug_strength --gamma=$gamma_value --mirror=$mirror_x --mirrory=False --nkimg=$train_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VznRirOE5ENI"
      },
      "source": [
        "## Convert Legacy Model\n",
        "\n",
        "If you have an older version of a model (Tensorflow based StyleGAN, or Runway downloaded .pkl file) you’ll need to convert to the newest version. If you’ve trained in this notebook you do **not** need to use this cell.\n",
        "\n",
        "`--source`: path to model that you want to convert\n",
        "\n",
        "`--dest`: path and file name to convert to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzkP-Rww5Np9",
        "cellView": "form"
      },
      "source": [
        "#@title Convert \n",
        "\n",
        "source = 'Path_to_model.pkl' #@param {type: 'string'}\n",
        "dest = 'Path_to_destination.pkl' #@param {type: 'string'}\n",
        "\n",
        "!python legacy.py --source=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/FreaGAN.pkl --dest=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/FreaGAN_ada.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6EtrPqL9ILk"
      },
      "source": [
        "## Testing/Inference\n",
        "\n",
        "Also known as \"Inference\", \"Evaluation\" or \"Testing\" the model. This is the process of usinng your trained model to generate new material, usually images or videos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a model to generate images from\n",
        "#@markdown Every script below will reference this model. To use a different model for any generation script, change this path.\n",
        "network_path = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00020-insects_machines-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000096.pkl' #@param {type: 'string'}"
      ],
      "metadata": {
        "id": "jJELM8ECvqHM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYdyfH0O8In_"
      },
      "source": [
        "### Generate Single Images\n",
        "\n",
        "`output`: folder to be created for current set of generated images. located in root/generated/ \n",
        "\n",
        "`seeds`: This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation. (format: single number: '234' list of numbers: '1,34,296' or range: '1-100'  no spaces in lists)\n",
        "\n",
        "`truncation`: Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYRXenMoZSHf",
        "cellView": "form"
      },
      "source": [
        "output = 'folder_name' #@param {type: 'string'}\n",
        "outdir = './generated/' + output +'/'\n",
        "seeds = '0-100' #@param {type: 'string'}\n",
        "truncation = 0 #@param {type: 'slider', min:0, max:1, step:0.1}\n",
        "\n",
        "\n",
        "\n",
        "!python generate.py --outdir=$outdir --trunc=$truncation --seeds=$seeds --network=$network_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjOTCWVonoVL"
      },
      "source": [
        "### Truncation Traversal\n",
        "\n",
        "Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n",
        "\n",
        "#### Options \n",
        "`--network`: Again, this should be the path to your .pkl file.\n",
        "\n",
        "`--seeds`: Pass this only one seed. Pick a favorite from your generated images.\n",
        "\n",
        "`--start`: Starting truncation value.\n",
        "\n",
        "`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n",
        "\n",
        "`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyzdGr7OnrMG",
        "cellView": "form"
      },
      "source": [
        "output_folder = 'trunc-trav-1' #@param {type:'string'} \n",
        "outdir = './generated/' + output_folder + '/'\n",
        "start =-5 #@param {type: 'slider', min:-5, max:5, step:0.1}\n",
        "stop = 5 #@param {type: 'slider', min:-5, max:5, step:0.1}\n",
        "increment = 0.5 #@param {type: 'slider', min:0, max:1, step:0.01}\n",
        "seeds = '250' #@param {type: 'string'}\n",
        "!python generate.py --process=\"truncation\" --outdir=$outdir --start=$start --stop=$stop --increment=$increment --seeds=$seeds --network=$network_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSzj0igO8Lfu"
      },
      "source": [
        "### Interpolations\n",
        "\n",
        "Interpolation is the process of generating very small changes to a vector in order to make it appear animated from frame to frame.\n",
        "\n",
        "We’ll look at different examples of interpolation below.\n",
        "\n",
        "#### Options\n",
        "\n",
        "\n",
        "\n",
        "`space`: z is more entangled (random) w is less entangled (more linear, smooth)\n",
        "\n",
        "`frames`: How many frames you want to produce. Use this to manage the length of your video.\n",
        "\n",
        "`seeds`: two or more seeds. (follow same format as mentioned above)\n",
        "\n",
        "`trunc`: truncation value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJSqafIzNwhx"
      },
      "source": [
        "#### Linear Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqkiskly8S5_",
        "cellView": "form"
      },
      "source": [
        "output_folder = 'folder_name' #@param {type: 'string'}\n",
        "outdir = outdir = './generated/' + output_folder + '/'\n",
        "space = 'z' #@param ['z', 'w']\n",
        "seeds = '1,34,2,10' #@param {type: 'string'}\n",
        "truncation = 5 #@param {type: 'slider', min:0, max:5, step:0.1}\n",
        "\n",
        "!python generate.py --outdir=$outdir --space=$space --trunc=$truncation --process=\"interpolation\" --seeds=$seeds --network=$network_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP1HsU_CPcF5"
      },
      "source": [
        "#### Noise Loop\n",
        "\n",
        "If you want to just make a random but fun interpolation of your model the noise loop is the way to go. It creates a random path thru the z space to show you a diverse set of images.\n",
        "\n",
        "`--interpolation=\"noiseloop\"`: set this to use the noise loop funtion\n",
        "\n",
        "`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n",
        "\n",
        "`--random_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times).\n",
        "\n",
        "Noise loops currently only work in z space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfR6DhfvN8b_",
        "cellView": "form"
      },
      "source": [
        "output_folder = 'folder_name' #@param {type: 'string'}\n",
        "outdir = outdir = './generated/' + output_folder + '/'\n",
        "diameter = 0 #@param {type: 'slider', min:0, max:10, step:0.1}\n",
        "frames = 720 #@param {type: 'number'}\n",
        "truncation = 0.2 #@param {type: 'slider', min:0, max:5, step:0.1}\n",
        "random_seed = 42 #@param {type: 'number'}\n",
        "\n",
        "!python generate.py --outdir=$outdir --trunc=$truncation --process=\"interpolation\" --interpolation=\"noiseloop\" --diameter=$diameter --frames=$frames --random_seed=$random_seed --network=$network_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKFb-4CedOq"
      },
      "source": [
        "#### Circular Loop\n",
        "\n",
        "The noise loop is, well, noisy. This circular loop will feel much more even, while still providing a random loop.\n",
        "\n",
        "I recommend using a higher `--diameter` value than you do with noise loops. Something between `50.0` and `500.0` alongside `--frames` can help control speed and diversity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao62za9_QfOF",
        "cellView": "form"
      },
      "source": [
        "output_folder = 'folder_name' #@param {type: 'string'}\n",
        "outdir = outdir = './generated/' + output_folder + '/'\n",
        "diameter = 0 #@param {type: 'slider', min:0, max:10, step:0.1}\n",
        "frames = 720 #@param {type: 'number'}\n",
        "truncation = 0.2 #@param {type: 'slider', min:0, max:5, step:0.1}\n",
        "random_seed = 42 #@param {type: 'number'}\n",
        "\n",
        "!python generate.py --outdir=$outdir --trunc=$truncation --process=\"interpolation\" --interpolation=\"circularloop\" --diameter=$diameter --frames=$frames --random_seed=$random_seed --network=$network_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz-fVtzyAHg1"
      },
      "source": [
        "## Projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez7tXSpCA_zh"
      },
      "source": [
        "### Basic Projector\n",
        "\n",
        "*   `--target`: this is a path to the image file that you want to \"find\" in your model. This image must be the exact same size as your model.\n",
        "*   `steps`: how many iterations the projctor should run for. Lower will mean less steps and less likelihood of a good projection. Higher will take longer but will likely produce better images.\n",
        "*    `seed`: random seed interger value (use only one number)\n",
        "*    `save_video`: Save an mp4 video of optimization progress\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80YTcjIQARWh",
        "cellView": "form"
      },
      "source": [
        "output_folder = 'bug' #@param {type: 'string'}\n",
        "outdir = './projection/output/' + output_folder + '/'\n",
        "target_image = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/projection/input/bug.png' #@param {type: 'string'}\n",
        "target = './projection/input/' + target_image + '.png' \n",
        "steps = 50 #@param {type: 'number'}\n",
        "seed = 0 #@param {type: 'number'}\n",
        "save_video = True #@param {type: 'boolean'}\n",
        "\n",
        "!python projector.py --network=$network_path --outdir=$outdir --target=$target_image --num-steps=$steps --seed=$seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF7RCnSAsWrq"
      },
      "source": [
        "## Feature Extraction using Closed Form Factorization\n",
        "\n",
        "Feature Extraction is the process of finding “human readable” vectors in a StyleGAN model. For example, let’s say you wanted to find a vector that could open or close a mouth in a face model.\n",
        "\n",
        "The feature extractor tries to automate the procss of finding important vectors in your model.\n",
        "\n",
        "`factor_name`: name of the feature vector file to output. (saved in ./factors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hek6TFZCKD-",
        "cellView": "form"
      },
      "source": [
        "#@title Generate Feature Vector File\n",
        "factor_name = 'factor_name' #@param {type: 'string'}\n",
        "out = './factors/' + factor_name + '.pt'\n",
        "\n",
        "!python closed_form_factorization.py --out=$out --ckpt=$network_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxLgeNeJRqFh"
      },
      "source": [
        "This process just created the vctor values, but we need to test it on some seed values to determine what each vector actually changes. The `apply_factor.py` script does this.\n",
        "\n",
        "Arguments to try:\n",
        "\n",
        "\n",
        "*   `i`: This stands for index. By default, the cell above will produce 512 vectors, so `-i` can be any value from 0 to 511. I recommend starting with a higher value.\n",
        "*   `d`: This stands for degrees. This means how much change you want to see along th vector. I recommend a value between 5 and 10 to start with.\n",
        "*   `seeds`: You know what these are by now right? :)\n",
        "*   `output`: where to save the images/video\n",
        "*   `space`: By default this will use the w space to reduce entanglement\n",
        "*   `factor_file`: path to .pt file created in the previous step\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEDSl2VpCSJL",
        "cellView": "form"
      },
      "source": [
        "i = 200 #@param {type: 'slider', min:0, max:511, step:1}\n",
        "d = 10 #@param {type: 'slider', min:1, max:100, step:1}\n",
        "seeds ='250' #@param {type: 'string'}\n",
        "output_folder = 'test_factors' #@param {type: 'string'}\n",
        "outdir = './generated/' + output_folder + '/'\n",
        "space = 'z' #@param ['z', 'w']\n",
        "factor_file = 'factor_name' #@param {type: 'string'}\n",
        "feature_pt = './factors/' + factor_file + '.pt'\n",
        "\n",
        "!python apply_factor.py -i $i -d $d --seeds $seeds --ckpt $network_path $feature_pt --output $outdir --video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzwhrjGlTMZ3"
      },
      "source": [
        "That just produced images or video for a single vector, but there are 511 more! To generate every vector, you can use the cell below. Update any arguments you want, `i` is set for you to the entire 0-511 range\n",
        "\n",
        "**Warning:** This takes a long time, especially if you have more than one seed value (pro tip: don’t usee more than one seed value)! Also, this will take up a good amount of space in Google Drive. You’ve been warned!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aFj6mcKDmqk",
        "cellView": "form"
      },
      "source": [
        "d = 10 #@param {type: 'slider', min:1, max:100, step:1}\n",
        "seeds ='250' #@param {type: 'string'}\n",
        "output_folder = 'test_factors' #@param {type: 'string'}\n",
        "outdir = './generated/' + output_folder + '/'\n",
        "space = 'z' #@param ['z', 'w']\n",
        "factor_file = 'factor_name' #@param {type: 'string'}\n",
        "feature_pt = './factors/' + factor_file + '.pt'\n",
        "\n",
        "for i in range(512):\n",
        "  !python apply_factor.py -i {i} -d $d --seeds $seeds --ckpt $network_path $feature_pt --output $outdir --video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VLRzmilrCf4"
      },
      "source": [
        "# Layer Manipulations\n",
        "\n",
        "The following scripts allow you to modify various resolution layers of the StyleGAN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDpQrBdevrDj"
      },
      "source": [
        "## Flesh Digressions\n",
        "\n",
        "Flesh Digressions works by manipulating the vectors in the base 4x4 layer. By doing this while leaving all the other layers untouched you can create a warping and twisting version of images from your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdvBNkMZv4MK",
        "cellView": "form"
      },
      "source": [
        "seed = '1' #@param {type: 'string'}\n",
        "truncation = 0.8 #@param {type: 'slider', min:0, max:1, step:0.1}\n",
        "\n",
        "!python flesh_digression.py --pkl $network_path --psi $truncation --seed $seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2TrnyvprL42"
      },
      "source": [
        "## Network Blending\n",
        "You can take two completely different models and combine them by splitting them at a specific resolution and combining the lower layers of one model and the higher layers of another.\n",
        "\n",
        "(Note: this tends to work best when one of the models is transfer learned from the other)\n",
        "\n",
        "*   `lower/higher_res_model`: paths to two differrent pkl files to combine\n",
        "*   `split res`: which layer to split models at. (lower layers are more structural while higher laryers are more textural)\n",
        "*   `output_name`: Name of new pkl file. (will be saved in ./pretrained/)\n",
        "recommended naming convention: \n",
        "\n",
        "    `name-of-lower-model`_`name-of-higher-model`_`split-res`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6pjl31Jwa4u",
        "cellView": "form"
      },
      "source": [
        "lower_res_model = 'path_to_pkl' #@param {type: 'string'}\n",
        "hgiher_res_model = 'path_to_pkl' #@param {type: 'string'}\n",
        "split_res = 64 #@param ['4', '16', '32', '64', '128', '256', '512']\n",
        "output_name = 'lower_upper_64' #@param {type: 'string'}\n",
        "output_path = './pretrained/' + output_name + '.pkl'\n",
        "\n",
        "!python blend_models.py --lower_res_pkl $lower_res_model --split_res $split_res --higher_res_pkl $hgiher_res_model --output_path $output_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "futaO6lBroVH"
      },
      "source": [
        "You can now take the output .pkl file and use that with any of the generation tools above."
      ]
    }
  ]
}