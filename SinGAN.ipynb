{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SinGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrober24/A.UD_289.84_seminar/blob/main/SinGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo6DvqDiB9C9"
      },
      "source": [
        "#SinGAN\n",
        "SinGAN is a ML library that allows you to do a lot of different things with one image as your dataset. As we’ve previously discussed usually you need a ton of images to be able to do anything in ML, so SinGAN needing only one image is a huge change.\n",
        "\n",
        "It is, however, important to keep in mind what its outputs are. SinGAN, to me, is most like Photoshop’s Context Aware tools. It allows you to manipulate the single image in interesting ways, but in the end is only going to return you that same image, slightly modified. SinGAN is a bit of a Swiss Army Knife and that makes it pretty cool in a world of single-use ML tools.\n",
        "\n",
        "Let’s take a look.\n",
        "\n",
        "Thanks to Derrick Schultz for much of the basis for this notebook find his SinGAN notebook here: [Derrick Schultz](https://github.com/dvschultz/SinGAN)\n",
        "\n",
        "Additional info on SinGAN:\n",
        "\n",
        "*   [SinGAN Github repo](https://github.com/tamarott/SinGAN)\n",
        "*   [ArXiv paper](https://arxiv.org/pdf/1905.01164.pdf)\n",
        "* [Supplementary Materials paper](https://tomer.net.technion.ac.il/files/2019/09/SingleImageGan_SM.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tatkCDnBqCH"
      },
      "source": [
        "## Set up SinGAN\n",
        "Let’s install the SinGAN repo and install all the dependencies needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcZrYaiYdZM5",
        "cellView": "form"
      },
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx1w_rUCAUNB",
        "cellView": "form"
      },
      "source": [
        "#@title Clone SinGAN Repo\n",
        "\n",
        "!git clone https://github.com/dvschultz/SinGAN\n",
        "!pip install torch==1.4.0 torchvision==0.5.0\n",
        "%cd /content/SinGAN/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMvS1d2mDxqA"
      },
      "source": [
        "##Training your image\n",
        "In order to use SinGAN, we need to train the model on our image. In my experience this usually takes an hour or so to train a 250px image.\n",
        "\n",
        "The `main_train.py` script takes one required argument, `--input_name`. To use with a custom image, upload an image to the `Input/Images` folder and then pass its name and file extension to the argument. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doXwNrKBD4V8",
        "cellView": "form"
      },
      "source": [
        "#@title Train\n",
        "\n",
        "input_img= 'test.png' #@param {type:'string'}\n",
        "max_size = 256 #@param ['256', '512', '1024']\n",
        "max = int(max_size)\n",
        "\n",
        "!python main_train.py --input_name $input_img --max_size $max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ISneFwF9Lfd"
      },
      "source": [
        "##Generating Random Samples\n",
        "By default, once training is completed 50 samples are generated. That means you don’t need to run the next line of code, unless you want to change some of the default options. \n",
        "\n",
        "Note: If you do run the next line of code, you’ll need to delete the folder of output images that were created during training. Otherwise you will get an error.\n",
        "\n",
        "###Sample Options\n",
        "\n",
        "####Number of Images\n",
        "`--num_samples` (note: this only works in my version of the library)\n",
        "\n",
        "You can pass in a number to generate any number of images. By default it generates 50. I recommend using this when running the `main_train.py` script otherwise you’ll need to delete the output folder and re-run it.\n",
        "\n",
        "####Generator scale\n",
        "`--gen_start_scale` (pass is a number, usually between 0–3)\n",
        "\n",
        "the generator scale relates to how \"realistic\" the new images look (similar to Truncation PSI in Stylegan). If you use `0` you will get images that differ drastically from your original image. If you use `2` or `3` the differences will be very subtle. Anything above these numbers will give you almost no changes.\n",
        "\n",
        "####Mode\n",
        "`--mode`\n",
        "There are two. While the default `random_samples` generates images of the same size as your training image, `random_samples_arbitrary_sizes` changes the size of your canvas. It’s not a scaled up image, however, its more like it makes the canvas larger and tries to fill the space added with a similar texture.\n",
        "\n",
        "####Scale of canvas\n",
        "`--scale_h` (pass it a floating number)\n",
        "\n",
        "`--scale_v` \n",
        "\n",
        "For use with `random_samples_arbitrary_sizes`, this allows you to define how much larger (or smaller) the canvas should be. you should pass it a floating value that scales each dimension up or down (`1.0` is the same size, `2.0` is twice as large, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9GXUNGUGnDk",
        "cellView": "form"
      },
      "source": [
        "#@title Generate Samples\n",
        "\n",
        "input_img= 'colosseum.png' #@param {type:'string'}\n",
        "gen_mode = 'random_samples' #@param ['random_samples', 'random_samples_arbitrary_sizes']\n",
        "gen_start_scale = 0 #@param {type: 'slider', min: 0, max: 3} \n",
        "num_samples = 50 #@param {type: 'slider', min: 1, max: 100}\n",
        "max_size = 256 #@param ['256', '512', '1024']\n",
        "max = int(max_size)\n",
        "scale_h = 1.0 #@param {type: 'slider', min: 1.0, max: 5.0, step: 0.1}\n",
        "scale_v = 1.0 #@param {type: 'slider', min: 1.0, max: 5.0, step: 0.1}\n",
        "\n",
        "!python random_samples.py --input_name $input_img --mode $gen_mode \\\n",
        " --gen_start_scale $gen_start_scale --num_samples $num_samples \\\n",
        " --max_size $max --scale_h $scale_h --scale_v $scale_v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFTvgNUigrE-"
      },
      "source": [
        "##Generating Animations\n",
        "SinGAN can generate an animation from the single image. It essentially create a noisy peturbation on the image causing it to animate.\n",
        "\n",
        "Unfortunately, this requires another training session on the image. It should take the same amount of time as your previous training.\n",
        "\n",
        "`--min_size` and `--max_size` can be used here as well.\n",
        "\n",
        "####Output type\n",
        "`--output_type`\n",
        "\n",
        "This allows you to output either `video` or `gif`. The default is `video`.\n",
        "\n",
        "`--num_frames`\n",
        "\n",
        "This determines the length of your animation. the default is `100` frames (10fps, so 10 seconds)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXHv8U14ZZ3q",
        "cellView": "form"
      },
      "source": [
        "#@title Animate\n",
        "\n",
        "input_img= 'colosseum.png' #@param {type:'string'}\n",
        "max_size = 256 #@param ['256', '512', '1024']\n",
        "max = int(max_size)\n",
        "output_type = 'video' #@param ['video', 'gif']\n",
        "frames = 120 #@param {type: 'number'}\n",
        "\n",
        "!python animation.py --input_name $input_img --max_size $max --output_type $output_type --num_frames $frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJbh5OP92KMI"
      },
      "source": [
        "You can play the video in Colab by pointing the path to the following function (you may need to update the width and height arguments as well):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluKyb_V0yvH",
        "cellView": "form"
      },
      "source": [
        "#@title Show Video\n",
        "\n",
        "vid_path = '/content/SinGAN/Output/Animation/colusseum/start_scale=2/alpha=0.100000_beta=0.800000.mp4' #@param {type: 'string'}\n",
        "W = 256 #@param {type: 'number'}\n",
        "H = 256 #@param {type: 'number'}\n",
        "\n",
        "def show_local_mp4_video(file_name, width=640, height=480):\n",
        "  import io\n",
        "  import base64\n",
        "  from IPython.display import HTML\n",
        "  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
        "  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
        "                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
        "                      </video>'''.format(width, height, video_encoded.decode('ascii')))\n",
        "\n",
        "show_local_mp4_video(vid_path, width= W, height= H)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}